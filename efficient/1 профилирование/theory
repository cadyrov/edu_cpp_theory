Зачем нужна профилировка
Как вы знаете, одно из самых важных преимуществ C++ перед другими современными языками программирования — скорость. Синтаксис вряд ли можно объективно назвать сильной стороной: кому-то он нравится, кому-то нет. А вот скорость программ, написанных на C++, на высоте. Тут ему на самом деле нет равных. Однако если писать синтаксически правильный, но плохой код, никакие преимущества C++ не сделают программу быстрой. Пройдя этот урок, вы узнаете, как находить проблемы производительности.
Профилировка
Рассмотрим такой код:
// Заголовок cstdlib понадобится для функции rand,
// выдающей случайные числа.
#include <cstdlib>
#include <iostream>
#include <vector>
using namespace std;
vector<int> ReverseVector(const vector<int>& source_vector) {
    vector<int> res;
    for (int i : source_vector) {
        res.insert(res.begin(), i);
    }
    return res;
}
// Функция считает количество ненулевых чисел в массиве
int CountPops(const vector<int>& source_vector, int begin, int end) {
    int res = 0;
    for (int i = begin; i < end; ++i) {
        if (source_vector[i]) {
            ++res;
        }
    }
    return res;
}

void AppendRandom(vector<int>& v, int n) {
    for (int i = 0; i < n; ++i) {
        // Получаем случайное число с помощью функции rand.
        // Конструкцией (rand() % 2) получим целое число в диапазоне 0..1.
        // В C++ имеются более современные генераторы случайных чисел,
        // но в данном уроке не будем их касаться.
        v.push_back(rand() % 2);
    }
}

int main() {
    vector<int> random_bits;

    // Операция << для целых чисел это сдвиг всех бит в двоичной
    // записи числа. Запишем с её помощью число 2 в степени 17 (131072)
    static const int N = 1 << 17;

    // Заполним вектор случайными числами 0 и 1.
    AppendRandom(random_bits, N);

    // Перевернём вектор задом наперёд.
    vector<int> reversed_bits = ReverseVector(random_bits);

    // Посчитаем процент единиц на начальных отрезках вектора.
    for (int i = 1, step = 1; i <= N; i += step, step *= 2) {
        // Чтобы вычислить проценты мы умножаем на литерал 100. типа double.
        // Целочисленное значение функции CountPops при этом автоматически
        // преобразуется к double, как и i.
        double rate = CountPops(reversed_bits, 0, i) * 100. / i;
        cout << "After "s << i << " bits we found "s << rate << "% pops"s
             << endl;
    }
}
Эта программа иллюстрирует важное следствие математического закона больших чисел: чем длиннее случайная последовательность нулей и единиц, тем больше вероятность, что процент единиц будет близок к 50%. Реверсирование вектора чисел в этой программе добавлено искусственно, чтобы потренироваться профилировать и оптимизировать код.
Запустим код и посмотрим, сколько времени займёт выполнение. Скорость зависит в том числе от оборудования. Но этот код в любом случае выполнится не моментально. Разберёмся, в чём проблема.
Для поиска узких мест проводят операцию профилирования — выборочное измерение времени выполнения участков кода. Есть несколько готовых профилировщиков — отдельных инструментов, показывающих, сколько времени работала функция. Они подключаются к вашей программе и очень часто — например, раз в миллисекунду — просматривают стек вызовов, проверяя, где выполнение происходит в данный момент. Анализируя данные профилировщика, вы легко найдёте узкие места — медленные функции, в которых программа проводит больше всего времени.
Некоторые известные профилировщики:
Intel®VTune™Profiler (Windows, Linux). Сложный коммерческий продукт в составе большого пакета инструментов Intel, предназначенных для анализа производительности. Имеет графический интерфейс для просмотра и составления различных отчётов о работе кода.
Microsoft Visual Studio (Windows). Начиная с версии 2017, в Microsoft Visual Studio входит собственный несложный профилировщик. Как и профилировщик Intel, он имеет интуитивный графический интерфейс.
Консольные инструменты (Linux). В ряде UNIX-подобных операционных систем есть инструменты strace, ltrace и gprof. Первые два служат для профилировки системных и библиотечных вызовов. Эти утилиты можно использовать с любой программой, они даже не требуют наличия в ней отладочной информации. gprof — это полноценный профилировщик, который может анализировать вашу программу. Он выдаёт результат в виде текстового файла.
На практике прибегать к сложным инструментам профилировки нужно не всегда. Поэтому вы напишете свой простой профилировщик. Он будет не сторонним инструментом, а встроится прямо в ваш код. Иногда такой профилировщик эффективнее, ведь он измеряет не всё, а только то, что вам нужно.
Определите узкое место этой программы. В ней три операции: заполнение вектора случайными числами, реверсирование и подсчёт единиц. Можно действовать так:
Вставьте код из теории в свою IDE.
По очереди убирая каждую из операций и запуская программу, найдите операцию, которая занимает больше всего времени.

Эта реализация реверсирования vector неэффективна, она отнимает больше всего времени.
Когда в программе всего три операции, можно действовать, как было предложено, и не профилировать. Хотя, вероятно, вы уже столкнулись с проблемами: удаление реверсирования влияет на следующий код. В реальных программах «подозреваемых» много, и наивными методами не обойтись — искать узкое место в объемном коде трудно. Даже в нашей маленькой программе пока непонятно, какой вклад в общее время вносит каждая из операций. Об этом и о том, как программы ускорять, — в следующих уроках.
Оптимизатор
Когда компилятор завершает работу, в дело вступает встроенный оптимизатор. Он сделает всё, чтобы ускорить программу, не испортив её. Оптимизатор может многое: удалять код, не влияющий на работу программы, подставлять тело функции в месте, где она вызывается, менять местами операции, устранять избыточные копирования. Шагая отладчиком по коду программы, вы могли заметить, что выполнение проходит не совсем так, как задумано. Это потому что поработал оптимизатор.
Вы можете влиять на оптимизатор, настраивая параметры компиляции. Основных вариантов два:
совсем отключить его. Так обычно делают для Debug-версий, поскольку оптимизатор может сильно усложнять отладку;
выбрать максимальную оптимизацию в Release-сборке.
Оптимизатор C++ — сложная вещь, и в некоторых случаях его поведение совсем не очевидно. Но даже самый изощренный оптимизатор не сделает из плохого и медленного кода быстрый и эффективный. Возможности оптимизатора широки, но не безграничны.
Оптимизация перепутает функции вашей программы, как спагетти. Некоторые функции она и вовсе удалит, встроив их код в другие. Это в свою очередь влияет на наглядность результатов профилировщика. В вашем профилировщике не будет такого недостатка. Но чтобы создать профилировщик, нужно овладеть временем, а вернее, механизмами его измерения в C++. Их вы изучите в следующем уроке.
Измеряем время
Главная задача профилировщика — измерять время выполнения определённых операций. Алгоритм измерения прост:
запомнить начальный момент (до операции),
запомнить конечный момент (после операции),
вычесть из конечного значения времени начальное.
В итоге получится продолжительность операции.

В C++ для этого есть замечательная библиотека <chrono>. Вы приме́ните эту библиотеку, создавая свой профилировщик.
Посмотрим, как работает <chrono>. Измерим время пяти секундного ожидания. Чтобы заставить программу подождать, используют функцию this_thread::sleep_for из библиотеки <thread>. Эту библиотеку вы изучите позже. Сейчас из неё понадобится только функция ожидания, останавливающая программу на заданное время:
#include <chrono>
#include <iostream>
#include <thread>

using namespace std;

int main() {
    cout << "Ожидание 5s..."s << endl;
    const chrono::steady_clock::time_point start_time = chrono::steady_clock::now();

    // операция - ожидание 5 секунд
    this_thread::sleep_for(chrono::seconds(5));
    const chrono::steady_clock::time_point end_time = chrono::steady_clock::now();

    const chrono::steady_clock::duration dur = end_time - start_time;
    cout << "Ожидание завершено"s << endl;
}
В выводе программы пока нет ничего содержательного:
Ожидание 5s...
Ожидание завершено
Чтобы узнать время, в C++, как и в обычной жизни, нужно обратиться к часам. В <chrono> есть разные виды часов: одни показывают привычное нам время, другие — время с момента запуска компьютера. Есть даже часы, используемые системами позиционирования GPS. Для измерения временных интервалов идеально подходят  chrono::steady_clock. Часы отсчитывают величину, называемую моментом времени. А разность моментов времени — это продолжительность. time_point и duration — разные типы данных, поэтому вы не сможете случайно сложить два момента времени вместо того, чтобы вычесть. Вы также не можете использовать момент времени там, где нужна продолжительность, и наоборот. Здесь мы указали типы полностью, а в дальнейшем для краткости будем писать auto.
Приведем продолжительность в понятный вид — например, в миллисекунды. Для этого применим функцию chrono::duration_cast. Мы пишем профилировщик — служебную часть программы, напрямую не связанную с её работой, поэтому результат будем выводить не в cout, а в cerr. Этот поток вывода при желании можно отделить от cout, перенаправив его в отдельный лог-файл с отладочной информацией:
#include <chrono>
#include <iostream>
#include <thread>

using namespace std;
using namespace chrono;

int main() {
    cout << "Ожидание 5s..."s << endl;
    const auto start_time = steady_clock::now();

    // операция - ожидание 5 секунд
    this_thread::sleep_for(seconds(5));
    const auto end_time = steady_clock::now();

    const auto dur = end_time - start_time;
    cerr << "Продолжительность сна: "s << duration_cast<milliseconds>(dur).count() << " ms"s << endl;

    cout << "Ожидание завершено"s << endl;
}
Вот какой результат может получиться:
Ожидание 5s...
Продолжительность сна: 5002 ms
Ожидание завершено
Запустим программу, перенаправляя cerr в файл. Если считать, что мы назвали исполняемый файл measure, то это делается такой командой:
measure.exe 2>err.txt
Или под UNIX-системами:
./measure 2>err.txt
Тогда вывод сократится:
Ожидание 5s...
Ожидание завершено
Зато появится файл err.txt с содержимым
Продолжительность сна: 5002 ms
Код можно упростить, если подключить пространство имён std::literals:
#include <chrono>
#include <iostream>
#include <thread>

using namespace std;
using namespace chrono;
// хотите немного магии? тогда используйте namespace literals
using namespace literals;

int main() {
    cout << "Ожидание 5s..."s << endl;
    const auto start_time = steady_clock::now();

    // операция - ожидание 5 секунд
    this_thread::sleep_for(5s);
    const auto end_time = steady_clock::now();

    const auto dur = end_time - start_time;
    cerr << "Продолжительность сна: "s << duration_cast<chrono::milliseconds>(dur).count() << " ms"s << endl;

    cout << "Ожидание завершено"s << endl;
}
Мы написали просто 5s, и компилятор понял, что мы имели в виду пять секунд! std::literals позволяет использовать различные единицы измерения времени от наносекунд (ns) до часов (h). Причём единицы не обязательно должны быть целыми: иногда нужно подождать полсекунды.
Вам могло показаться странным, что тип длины интервала chrono::steady_clock::duration зависит от типа часов.
Часы с определённой периодичностью отсчитывают тики, прошедшие с некоторого начального момента, который называется «эпоха». Эти тики происходят в некоторые моменты времени, задаваемые типом chrono::{тип_часов}::time_point. Интервал между двумя любыми моментами времени на часах задаётся типом chrono::{тип_часов}::duration.
Каждые часы задают свою систему отсчёта времени: эпоху и периодичность тиков. В этой системе отсчёта задаются моменты времени. Приведем аналогию с часами из реальной жизни:
Настенные часы показывают время 8:15:13 после полуночи. Период тиков — 1 секунда.
Часы на духовом шкафу показывают 20:16 с начала приготовления пирога. Период тиков — 1 минута.
Секундомер показывает время 30:25,17 с начала забега. Период тиков — 1/100 секунды.
Библиотека <chrono> разрешает находить разницу между моментами времени, только если они получены с одних и тех же часов. Моменты времени с разных часов никак друг с другом не связаны. И действительно, вопрос «сколько времени прошло между моментом 3:15:00 на настенных часах и моментом 1:10:53 на секундомере» не имеет смысла, если не знаем время на настенных часах, когда секундомер показывал 0:00. Более того, ответ на вопрос зависит от того, переводились ли настенные часы на летнее или зимнее время в этот промежуток времени.
В <chrono> есть универсальный тип — шаблонный класс  chrono::duration. Его шаблонные параметры задают:
числовой тип, измеряющий количество тиков. Может быть целым, как int и int64_t, или вещественным, как float или double.
продолжительность одного тика, задаваемого в виде дробного количества секунд. Например, 1/1000. Чтобы задать продолжительность, используется шаблонный класс ratio, позволяющий задавать рациональные числа.
Примеры:
chrono::seconds — это часто то же самое, что и chrono::duration<int64_t>,
chrono::milliseconds аналогично chrono::duration<int64_t, chrono::milli>,
chrono::minutes аналогично chrono::duration<int64_t, ratio<60>>.
Каждые часы определяют наиболее подходящие типы для хранения количества и продолжительности тиков.
Упрощаем логирование
Выполняя задание в прошлом уроке, вы могли заметить, что расставлять отметки времени начала и конца каждой операции — довольно утомительная работа. А главное — в ней легко ошибиться и перепутать многочисленные переменные типа time_point. К счастью, в C++ есть механизм, который позволяет выполнять парные действия удобно и без риска ошибок.
Создадим класс LogDuration, объект которого при создании и уничтожении будет запоминать время. При уничтожении он выведет продолжительность операции в cerr:
class LogDuration {
public:
    LogDuration() {
    }

    ~LogDuration() {
        const auto end_time = steady_clock::now();
        const auto dur = end_time - start_time_;
        cerr << duration_cast<milliseconds>(dur).count() << " ms"s << endl;
    }

private:
    // В переменной будет время конструирования объекта LogDuration
    const steady_clock::time_point start_time_ = steady_clock::now();
};
LogDuration упростит профилировку. Надо просто создать переменную типа LogDuration с любым названием, и она автоматически измерит время, прошедшее от своего создания до выхода из области видимости. Рассмотрим работу класса LogDuration на примере:
#include <chrono>
#include <iostream>
#include <thread>

using namespace std;
using namespace chrono;
using namespace literals;

class LogDuration {
public:
    LogDuration() {
    }

    ~LogDuration() {
        // эта переменная сохранит время уничтожения объекта:
        const auto end_time = steady_clock::now();
        const auto dur = end_time - start_time_;
        cerr << duration_cast<milliseconds>(dur).count() << " ms"s << endl;
    }

private:
    // Переменная будет инициализирована текущим моментом времени при
    // создании объекта класса.
    // Таким образом, она хранит время создания объекта.
    const steady_clock::time_point start_time_ = steady_clock::now();
};

int main() {
    cout << "Ожидание 5s..."s << endl;

    {
        LogDuration sleep_guard;
        // операция - ожидание 5 секунд
        this_thread::sleep_for(5s);
    }

    cout << "Ожидание завершено"s << endl;
}
Запустим программу и убедимся в корректной работе LogDuration:
Ожидание 5s...
5000 ms
Ожидание завершено
Всё так, как мы и ожидали. Пора опробовать новый класс в действии.
Призываем макросы
В прошлом уроке вы упростили профилировку. Технология, которую вы использовали, называется RAII. Она применяется не только для контроля за ресурсами, но и для планирования действия, которое должно совершиться при выходе из области видимости.
Название переменной, которая позволила создать объект LogDuration, не несёт в себе никакой информации. Более того, мы даже не обращаемся к этой переменной. Чтобы скрыть ненужную информацию, используем препроцессор. С ним вы уже работали. Определим такой макрос:
#define LOG_DURATION(x) LogDuration UNIQUE_VAR_NAME_PROFILE(x)
Далее нужно разработать макрос UNIQUE_VAR_NAME_PROFILE, который будет выдавать уникальное имя переменной. Для профилировщика это не так важно — вряд ли у нас будут две профилировки в одном пространстве имён. Но всё-таки это удобно.
Можем использовать макрос __LINE__. Препроцессор заменяет его на номер строки, в которой использован этот макрос. Нам также понадобится оператор слияния лексем ##:
#define UNIQUE_VAR_NAME_PROFILE profile_guard_ ## __LINE__

int main() {
    int UNIQUE_VAR_NAME_PROFILE;
}
Забавно, в этой программе нет #include, при этом она совершенно корректна, хотя ничего не делает — даже единственную переменную исключит оптимизатор. Посмотрим, что на выходе у препроцессора:
#line 1 "main.cpp"
int main() {
    int profile_guard___LINE__;
}
Этот файл мы получили, применив специальный режим компилятора. В таком режиме он запускает только препроцессор. Для компилятора GCC этот режим можно запустить из командной строки, указав специальный флаг -E:
gcc -E main.cpp -o main.i
Ожидали мы не этого. Очевидно, препроцессор «приклеил» __LINE__ слишком рано, до того, как подставил номер строки. Чтобы подставить значение __LINE__ до приклеивания к profile_guard_, вынесем слияние лексем в отдельное макроопределение, PROFILE_CONCAT:
#define PROFILE_CONCAT(X, Y) X ## Y
#define UNIQUE_VAR_NAME_PROFILE PROFILE_CONCAT(profile_guard_, __LINE__)

int main() {
    int UNIQUE_VAR_NAME_PROFILE;
}
Код стал на одну строчку больше. Проверим результат:
#line 1 "main.cpp"

int main() {
    int profile_guard___LINE__;
}
Результат не изменился. Почти смирившись, что ничего не выйдет, сделаем последнюю попытку. Добавим ещё один макрос:
#define PROFILE_CONCAT_INTERNAL(X, Y) X ## Y
#define PROFILE_CONCAT(X, Y) PROFILE_CONCAT_INTERNAL(X, Y)
#define UNIQUE_VAR_NAME_PROFILE PROFILE_CONCAT(profile_guard_, __LINE__)

int main() {
    int UNIQUE_VAR_NAME_PROFILE;
}
Посмотрим, что выдал препроцессор:
#line 1 "main.cpp"

int main() {
    int profile_guard_6;
}
Ничего себе, получилось!
📖 Такого трюка для определения UNIQUE_VAR_NAME_PROFILE — макроса, генерирующего уникальное имя переменной — требуют довольно запутанные правила раскрытия в C++. Параметры макроса при склеивании заменяются на то, что в них было оставлено без изменения. Те параметры, которые не склеиваются, раскрываются, то есть полностью подставляются до того момента, пока в них не останется макросов. Чтобы достичь желаемого, нужно, чтобы __LINE__ побывал параметром два раза: в первый раз он раскроется в номер строки, во второй раз номер строки приклеится к имени переменной.
Окончательный вид будет таким:
#define PROFILE_CONCAT_INTERNAL(X, Y) X ## Y
#define PROFILE_CONCAT(X, Y) PROFILE_CONCAT_INTERNAL(X, Y)
#define UNIQUE_VAR_NAME_PROFILE PROFILE_CONCAT(profileGuard, __LINE__)
#define LOG_DURATION(x) LogDuration UNIQUE_VAR_NAME_PROFILE(x)
Измеряем и ускоряем
Вы потратили много усилий на макросы и теперь ваш профилировщик готов. Посмотрим, на что он способен.
Возможно, вы догадались, почему в примере из прошлого урока так много времени занимает реверсирование вектора. А если не догадались — не страшно, сейчас разберёмся.
Дело в том, что когда вектор достаточно велик, вставка элементов в начало и середину — долгая операция: нужно подвинуть много элементов вправо, чтобы освободить ячейку. А мы делаем эту операцию много раз. Куда быстрее вставлять в конец — вспомните пример с чемоданом из начала курса:



Чтобы положить что-то вниз чемодана, как и в начало вектора, нужно переместить всё содержимое
Возьмем код из предыдущего урока. Изменим программу так, чтобы вставка производилась в конец. Раньше мы читали с начала и вставляли в начало. А теперь будем читать с конца и вставлять в конец. Вроде бы ничего не изменилось, но теория подсказывает, что вставка в конец эффективнее.
Чтобы изменить направление прохода по вектору, используем обратные итераторы rbegin и rend. Они работают так же, как begin и end, но проходят контейнер в обратном направлении:
vector<int> ReverseVector2(const vector<int>& source_vector) {
    vector<int> res;

    // будем проходить source_vector задом наперёд
    // с помощью обратного итератора
    for (auto iterator = source_vector.rbegin(); iterator != source_vector.rend(); ++iterator) {
        res.push_back(*iterator);
    }

    return res;
}
Не забываем в main поменять ReverseVector на ReverseVector2 и измеряем время.
Было
Стало
Append random: 3 ms
Append random: 6 ms
Reverse: 2402 ms
Reverse: 1 ms
Counting: 38 ms
Counting: 37 ms
Total: 2446 ms
Total: 46 ms

Круто, производительность всей программы возросла более чем в 50 раз! Попробуем ускорить ещё. Но чтобы разница была заметной, увеличим размер вектора в 256 раз: с 2
17
до 2
25
или 33 554 432:
static const int N = 1 << 25;
Теперь измерения выглядят так:
Стало
Append random: 1366 ms
Reverse: 549 ms
Counting: 80 ms
Total: 2019 ms

Начинаем ускорять с "Append random" — самой медленной части, потому что она вносит основной вклад в скорость программы. Ведь даже если мы очень постараемся и ускорим Counting в 100 раз, производительность всей программы улучшится только на 4%, а если в 100 раз ускорить Append random, то на 67%. Разница очевидна.
При случайном заполнении вектора требуется только один бит информации, а rand() выдает как минимум 15 случайных бит. Сколько точно — зависит от операционной системы и компилятора. Используем все 15, которые нам гарантированы:
// <algorithm> нужен для функции min
#include <algorithm>
... 
void AppendRandom2(vector<int>& v, int n) {
    for (int i = 0; i < n; i += 15) {
        int number = rand();

        // мы можем заполнить 15 элементов вектора,
        // но не более, чем нам осталось до конца:
        int count = min(15, n - i);

        for (int j = 0; j < count; ++j)
            // таким образом, получим j-й бит числа.
            // операцию побитового сдвига вы уже видели в этой программе
            // на этот раз двигаем вправо, чтобы нужный бит оказался самым последним
            v.push_back((number >> j) % 2);
    }
}
...
Было
Стало
Append random: 1366 ms
Append random: 620 ms
Reverse: 549 ms
Reverse: 579 ms
Counting: 80 ms
Counting: 81 ms
Total: 2019 ms
Total: 1312 ms

Ускорение уже скромнее. Однако это лучше, чем ничего. Мы договорились ускорять самую медленную часть программы, но идей, как оптимизировать "Append random" пока нет. Поэтому улучшим функцию Reverse, вторую по скорости. Вставлять элемент в конец вектора эффективнее, чем в начало, но можно еще ускорить процесс. Для этого зарезервируем место — применим метод reserve, указав количество элементов, которые будут в векторе в итоге:
vector<int> ReverseVector3(const vector<int>& source_vector) {
    vector<int> res;
    res.reserve(source_vector.size());

    // будем проходить sourceVector задом наперёд
    // с помощью обратного итератора
    for (auto iterator = source_vector.rbegin(); iterator != source_vector.rend(); ++iterator) {
        res.push_back(*iterator);
    }

    return res;
}
Было
Стало
Append random: 620 ms
Append random: 624 ms
Reverse: 579 ms
Reverse: 194 ms
Counting: 81 ms
Counting: 80 ms
Total: 1312 ms
Total: 926 ms

Отлично! Не так хорошо, как при первой оптимизации, но всё-таки ускорение почти в три раза — это очень неплохо.
Проверяем, всё ли ускорили
В прошлом уроке мы проделали неплохую работу: улучшили скорость работы вначале в 50 раз, а затем еще минимум в два раза. Казалось бы, методы ускорения исчерпаны. Однако последнюю операцию — подсчёт количества единиц — мы пока не трогали. Хоть она занимает всего около 10% времени программы, поработаем над ней тоже, просто чтобы ей не было обидно.
Вспомним, что у нас есть к этому моменту:
#include <algorithm>
#include <chrono>
#include <cstdlib>
#include <iostream>
#include <vector>

#include "log_duration.h"

using namespace std;

vector<int> ReverseVector3(const vector<int>& source_vector) {
    return {source_vector.rbegin(), source_vector.rend()};
}

int CountPops(const vector<int>& source_vector, int begin, int end) {
    int res = 0;

    for (int i = begin; i < end; ++i) {
        if (source_vector[i]) {
            ++res;
        }
    }

    return res;
}

void AppendRandom2(vector<int>& v, int n) {
    for (int i = 0; i < n; i += 15) {
        int number = rand();

        // мы можем заполнить 15 элементов вектора,
        // но не более, чем нам осталось до конца:
        int count = min(15, n - i);

        for (int j = 0; j < count; ++j)
            // операцию сдвига битов вы уже видели в этой программе
            // на этот раз двигаем вправо, чтобы нужный бит оказался самым последним
            v.push_back((number >> j) % 2);
    }
}

void Operate() {
    LOG_DURATION("Total"s);
    vector<int> random_bits;
    // Операция << для целых чисел - это сдвиг всех бит в двоичной
    // записи числа. Запишем с её помощью число 2 в степени 17 (131072).
    static const int N = 1 << 17;
    // заполним вектор случайными числами 0 и 1
    {
        LOG_DURATION("Append random"s);
        AppendRandom2(random_bits, N);
    }
    // перевернём вектор задом наперёд
    vector<int> reversed_bits;
    {
        LOG_DURATION("Reverse"s);
        reversed_bits = ReverseVector3(random_bits);
    }

    {
        LOG_DURATION("Counting"s);
        // посчитаем процент единиц на начальных отрезках вектора
        for (int i = 1, step = 1; i <= N; i += step, step *= 2) {
            double rate = CountPops(reversed_bits, 0, i) * 100. / i;
            cout << "After "s << i << " digits we found "s << rate << "% pops"s << endl;
        }
    }
}
Каждый раз суммируем весь диапазон, а могли бы просто добавлять к уже рассчитанной сумме новые слагаемые. Для этого нужно усложнить алгоритм:
{
    LOG_DURATION("Counting"s);
    // посчитаем процент единиц на начальных отрезках вектора
    int prev_sum = 0;
    int prev_i = 0;
    for (int i = 1, step = 1; i <= n; i += step, step *= 2) {
        const int sum = prev_sum + CountPops(reversed_bits, prev_i, i);

        cout << "After "s << i << " digits we found "s << (sum * 100. / i) << "% pops"s << endl;

        prev_i = i;
        prev_sum = sum;
    }
}
Было
Стало
Append random: 624 ms
Append random: 631 ms
Reverse: 194 ms
Reverse: 207 ms
Counting: 80 ms
Counting: 77 ms
Total: 926 ms
Total: 961 ms

А вот тут нас постигла неудача: лишние вычисления мы вроде бы убрали, однако результат изменился незначительно. Дело в том, что это суммирование занимает малую часть всего времени. Узкое место алгоритма — вывод в поток, а не суммирование. Но даже если вывод в поток ускорить, это не даст заметного выигрыша во всей программе. Об оптимизации потоков поговорим в следующей теме.
Два важных правила оптимизации
В предыдущем уроке нас постигла неудача: оптимизация не дала желаемого ускорения. Это показывает, что оптимизация — не всегда хорошо. Код стало сложнее писать и читать, а результат оказался неудовлетворительным. Чтобы избегать подобных неприятностей, следуйте первому правилу оптимизации.
💡 Первое правило оптимизации: избегайте преждевременной оптимизации.
Главные достоинства вашей программы — это надежность, понятность и скорость. Усложняя программу ради скорости, вы ухудшаете понятность, а скорее всего, и надежность, потому что в сложных и непонятных программах гораздо проще допустить ошибку. Особенно обидно, если результат того не стоил.
Если оптимизация сработала, и вы путём большого труда ускорили функцию, занимавшую 10 миллисекунд, до одной миллисекунды — это хорошее достижение. Но если вся программа при этом работает 10 секунд, ускорение попросту не будет заметно. Понять, где именно нужна оптимизация, поможет профилировка. Это второе правило оптимизации.
💡 Второе правило оптимизации: измеряйте.
Только измерения помогут выявить узкие места программы — те, где ваша программа в действительности проводит больше всего времени. Их расположение часто противоречит интуиции.
Есть даже эмпирическое правило Парето, согласно которому:
💡 80% всего времени выполнения программы тратится на 20% кода.
В реальных программах это соотношение может быть иным и часто даже более разительным. Но в любом случае оно говорит, что во время разработки стоит думать не только об эффективности кода, но и о том, что ему может понадобиться оптимизация.
Вглубь процессора
Оптимизация — это процесс, позволяющий заменить операции программы на более эффективные и тем самым ускорить вычисления на несколько процентов или в сотни раз. Разберемся, можно ли оптимизировать программу, не меняя операции, которые она совершает.
Представим, что в некоторой обсерватории есть телескоп. За одну ночь он совершает измерения на n участках неба. Результат измерения кодируется одним латинским символом от A до Z. В результате получается строка длины n.

Собраны результаты за длительный период работы телескопа. Среди них нужно найти участки неба, где больше всего сильных сигналов — с литерой K или далее. Реализуем функцию подсчёта сильных сигналов по каждому участку неба:
#include <string>
#include <vector>

using namespace std;

// Функция анализирует данные телескопа, определяя, сколько сильных сигналов
// зафиксировано по каждому направлению.
vector<int> ComputeStatistics(const vector<string>& measures, int n) {
    int m = measures.size();
    vector<int> result(n);

    for (int i = 0; i < n; ++i) {
        for (int j = 0; j < m; ++j) {
            if (measures[j][i] >= 'K') {
                ++result[i];
            }
        }
    }

    return result;
}
Функция возвращает вектор, элементы которого — это количества сильных сигналов на нулевом участке неба, первом, втором. Протестируем эту функцию, измерив время. Считаем, что прошло 5000 дней и количество участков неба тоже равно 5000. Реальных данных телескопа у нас под рукой нет, поэтому заполним вектор случайными буквами:
#include "log_duration.h"

#include <algorithm>
#include <iostream>
#include <numeric>
#include <random>
#include <string>
#include <vector>

using namespace std;

// функция анализирует данные телескопа, определяя, сколько сильных сигналов
// зафиксировано по каждому направлению
vector<int> ComputeStatistics(const vector<string>& measures, int n) {
    int m = measures.size();
    vector<int> result(n);

    for (int i = 0; i < n; ++i) {
        for (int j = 0; j < m; ++j) {
            // прибавляем 1, если сигнал не менее 'K'
            result[i] += (measures[j][i] >= 'K' ? 1 : 0);
        }
    }

    return result;
}

string GetRandomString(int size) {
    static mt19937 engine;
    uniform_int_distribution<int> distribution('A', 'Z');

    string res(size, ' ');
    for (char& c : res) {
        c = char(distribution(engine));
    }

    return res;
}

int main() {
    vector<string> data;

    for (int i = 0; i < 5000; ++i) {
        data.push_back(GetRandomString(5000));
    }

    vector<int> statistics;
    {
        LOG_DURATION("ComputeStatistics"s);
        statistics = ComputeStatistics(data, 5000);
    }

    cout << "Всего сильных сигналов: "s << accumulate(statistics.begin(), statistics.end(), 0) << endl;
}
Результат может быть таким:
ComputeStatistics: 134 ms
Всего сильных сигналов: 15379777
Подумаем, много это или мало — 134 миллисекунды. За такое время процедура обрабатывает 5000 × 5000, то есть 25 миллионов байт. Получается, что за секунду было бы обработано около 187 мегабайт. Сравним с другой программой, например, видеоплеером. Современные видеоплееры должны декодировать и показать в секунду минимум 25 кадров в разрешении 4K, то есть 4096 × 3072 × 25 байт. Это примерно 315 мегабайт. Что-то подсказывает, что декодирование — куда более сложная операция, чем простой подсчет количества определенных букв. Значит, в нашей программе что-то не так.
Внесем в программу минимальные изменения — поменяем порядок циклов в основной процедуре подсчета:
vector<int> ComputeStatistics2(const vector<string>& measures, int n) {
    int m = measures.size();
    vector<int> result(n);

    // теперь первым идёт цикл по j
    for (int j = 0; j < m; ++j) {
        for (int i = 0; i < n; ++i) {
            // прибавляем 1, если сигнал не менее 'K'
            result[i] += (measures[j][i] >= 'K' ? 1 : 0);
        }
    }

    return result;
}
Запустим эту программу и сравним две функции:
ComputeStatistics: 130 ms
ComputeStatistics2: 42 ms
Всего сильных сигналов: 15379777
Неожиданный результат: выигрыш в три раза. Наверное, мы что-нибудь перепутали, ведь процессору всё равно, в каком порядке выполнять сложения, если их количество не изменилось.
При оптимизации и другом рефакторинге нужно контролировать, что ответ оптимизированной функции будет таким же, как ответ старой, проверенной. Сделаем это. Если ответы совпадут, значит, ComputeStatistics2 выполняет свою задачу, причём делает это в разы быстрее. Вставим в main такой код:
...
vector<int> statistics2;
{
    LOG_DURATION("ComputeStatistics2"s);
    statistics2 = ComputeStatistics2(data, 5000);
}

cout << (statistics == statistics2 ? "OK"s : "Fail"s) << endl;
...
Запустим еще раз:
ComputeStatistics: 131 ms
ComputeStatistics2: 48 ms
OK
Всего сильных сигналов: 15379777
Ответы совпали! Чтобы понять, почему один алгоритм быстрее другого, нужно углубиться в то, как работают RAM и кэш процессора — специальная быстрая память, с которой процессор оперирует напрямую.
Данные из RAM помещаются в кэш не по одному байту, а блоками. Поэтому когда из объекта string читается один символ, все соседние тоже загружаются в кэш и доступ к ним происходит быстрее, чем к далёким символам и элементам других строк.
Фрагмент кэша, хранящий один блок, называется «кэш-линия». Процессоры имеют несколько кэш-линий. Их размер зависит от архитектуры процессора. У многих современных процессоров он равен 64 байтам. Каждая кэш-линия хранит участок значений, к которым недавно обращалась какая-либо работающая программа.
Если вы сразу обработаете все значения, попавшие в одну кэш-линию, процессору не придется перезаписывать кэш и обращаться к медленной памяти RAM каждый раз при чтении символа из строки.
Символы разных строк располагаются в памяти далеко и, скорее всего, не попадут в кэш-линию одновременно. Поэтому если программа попеременно обращается к разным строкам, процессор будет постоянно обновлять кэш-линии и простаивать, ожидая, пока данные из RAM попадут в кэш.
📖 Именно на особенностях кэша основана знаменитая уязвимость Meltdown, обнаруженная в большом количестве процессоров в 2017 году.
Когда внутренний цикл проходится по одной строке, её большой кусок загружается в кэш. Происходит работа над данными одной кэш-линии до тех пор, пока все они не будут обработаны. Если же внутренний цикл проходит «по вертикали», то из кэш-линии вы по сути обрабатываете только один символ.
Вы могли заметить, что прибавление единицы было странно реализовано через тернарный оператор. При этом явно происходят прибавления нуля — лишние сложения, которые ничего не меняют. Попробуем ускорить программу ещё, убрав их:
vector<int> ComputeStatistics3(const vector<string>& measures, int n) {
    int m = measures.size();
    vector<int> result(n);

    for (int j = 0; j < m; ++j) {
        for (int i = 0; i < n; ++i) {
            // будем прибавлять только когда надо
            if (measures[j][i] >= 'K') {
                ++result[i];
            }
        }
    }

    return result;
}
Надеемся на большее ускорение и запускаем:
ComputeStatistics: 142 ms
ComputeStatistics2: 41 ms
ComputeStatistics3: 198 ms
OK
Всего сильных сигналов: 15379777
Вот это дела! Мы уменьшили количество сложений, не добавив, казалось бы, новых операций, а скорость при этом упала почти в пять раз.
Причина такого поведения программы тоже в особенностях процессоров. if вставляет специальную процессорную инструкцию — условный переход. Эта инструкция — одна из самых сложных и неприятных для процессора. Он не может работать эффективно, когда не знает, что ему делать дальше. При условном переходе это неизвестно, поэтому процессор как бы впадает в ступор. В критических местах программы нужно избегать if, предпочитая тернарный оператор или другие средства. Но тернарный оператор и цикл тоже могут создавать инструкцию условного перехода. Они будут избегать её лишь в определённых простых случаях.
📖 В современные процессоры встроен так называемый предсказатель веток — алгоритм, который выбирает заранее одну из веток if/else на основе статистики и начинает выполнять её до того, как сделает проверку условия. Эта особенность лежит в основе уязвимости Spectre. Там использовался код с if, в котором всё время выполнялась одна и та же ветка. В определённый момент условие вдруг оказывалось ложным, но процессор как бы по инерции продолжал выполнять первую ветку. Окончательно условие проверялось позже, и процессор откатывал все изменения, произошедшие не в той ветке. Но последствия этих изменений всё равно могли быть обнаружены.
Выходит, самая эффективная реализация — вторая. В ней память обрабатывается последовательно, отсутствует if.
Мы проделали неплохую работу, сравнив разные реализации. Измерения действительно были полезны. Особенно когда мы были уверены, что улучшим производительность, убрав ненужные прибавления, а вышло наоборот. Этот пример показывает, что предусмотреть все нюансы нельзя, нужно измерять.
Реализованная процедура ComputeStatistics2 уже достаточно эффективна. Но можно применить ещё несколько оптимизаций, выходящих за рамки этого урока:
Специальные функции, которые позволяют производить одну и ту же операцию над несколькими числами одновременно, — в нашем случае сравнение и сложение. Эта технология называется Single Instruction Multiple Data или SIMD. О таких функциях можно почитать в гайде от Intel. Когда сайт недоступен или не работает, используйте его зеркало.
Разворачивание цикла. Дело в том, что for тоже содержит условный переход в начале каждой итерации. Для оптимизации можно обработать в одной итерации сразу несколько элементов — например 16. Так мы сократим количество итераций и, соответственно, условных переходов в 16 раз. Это достаточно спорный подход, так как оптимизаторы иногда делают разворачивание цикла автоматически, а понятность кода при этом ухудшается.
Распараллеливание. Можно поручить нескольким потокам выполнения обрабатывать разные участки вектора. Это должно существенно ускорить работу на многоядерных компьютерах, если другие ядра не задействованы. Когда компьютер одновременно считает статистику с нескольких телескопов, распараллеливание вряд ли даст выигрыш.

# Дополнительные материалы для профилирования и оптимизации

## Лучшие практики профилирования

### ✅ Хорошие практики:

1. **Профилирование на Release сборке**
   - Всегда профилируйте оптимизированный код (-O2 или -O3)
   - Debug версии могут показать неточную картину производительности

2. **Использование правильных часов**
   ```cpp
   // Хорошо: steady_clock для измерения интервалов
   auto start = std::chrono::steady_clock::now();
   // операция
   auto end = std::chrono::steady_clock::now();
   auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);
   ```

3. **RAII для профилирования**
   ```cpp
   // Хорошо: автоматическое измерение времени
   class ScopedTimer {
       std::chrono::steady_clock::time_point start_;
       std::string name_;
   public:
       ScopedTimer(const std::string& name) : name_(name), start_(std::chrono::steady_clock::now()) {}
       ~ScopedTimer() {
           auto end = std::chrono::steady_clock::now();
           auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start_);
           std::cerr << name_ << ": " << duration.count() << " μs\n";
       }
   };
   ```

4. **Измерение статистически значимых данных**
   ```cpp
   // Хорошо: несколько измерений для точности
   std::vector<long long> measurements;
   for (int i = 0; i < 100; ++i) {
       auto start = std::chrono::steady_clock::now();
       // операция
       auto end = std::chrono::steady_clock::now();
       measurements.push_back(std::chrono::duration_cast<std::chrono::microseconds>(end - start).count());
   }
   // Анализ медианы, среднего, стандартного отклонения
   ```

5. **Правильное использование компилятора**
   ```bash
   # Хорошо: оптимизированная сборка для профилирования
   g++ -O2 -DNDEBUG -march=native -mtune=native source.cpp
   ```

### ❌ Плохие практики:

1. **Профилирование Debug версий**
   ```cpp
   // Плохо: неоптимизированный код даёт неточные результаты
   #ifdef DEBUG
   // профилирование здесь бесполезно
   #endif
   ```

2. **Использование неточных часов**
   ```cpp
   // Плохо: system_clock может изменяться
   auto start = std::chrono::system_clock::now();
   // операция
   auto end = std::chrono::system_clock::now();
   ```

3. **Игнорирование разогрева кэша**
   ```cpp
   // Плохо: первое измерение может быть неточным
   auto start = std::chrono::steady_clock::now();
   process_data(data); // холодный кэш
   auto end = std::chrono::steady_clock::now();
   ```

4. **Профилирование слишком коротких операций**
   ```cpp
   // Плохо: операция слишком быстрая для точного измерения
   auto start = std::chrono::steady_clock::now();
   int x = 5 + 3; // наносекунды
   auto end = std::chrono::steady_clock::now();
   ```

## Оптимизация производительности

### ✅ Хорошие практики:

1. **Локальность данных**
   ```cpp
   // Хорошо: последовательный доступ к памяти
   for (size_t i = 0; i < rows; ++i) {
       for (size_t j = 0; j < cols; ++j) {
           matrix[i][j] = process(matrix[i][j]);
       }
   }
   ```

2. **Избегание ненужных копирований**
   ```cpp
   // Хорошо: передача по константной ссылке
   void process(const std::vector<int>& data) {
       // обработка без копирования
   }
   
   // Хорошо: move семантика
   std::vector<int> result = std::move(temp_vector);
   ```

3. **Резервирование памяти**
   ```cpp
   // Хорошо: избегаем реаллокации
   std::vector<int> vec;
   vec.reserve(expected_size);
   for (int i = 0; i < expected_size; ++i) {
       vec.push_back(i);
   }
   ```

4. **Использование констант времени компиляции**
   ```cpp
   // Хорошо: вычисления на этапе компиляции
   constexpr int factorial(int n) {
       return n <= 1 ? 1 : n * factorial(n - 1);
   }
   constexpr int fact5 = factorial(5); // вычисляется во время компиляции
   ```

5. **Правильное использование алгоритмов STL**
   ```cpp
   // Хорошо: использование оптимизированных алгоритмов
   std::sort(vec.begin(), vec.end());
   auto it = std::lower_bound(vec.begin(), vec.end(), target);
   ```

### ❌ Плохие практики:

1. **Плохая локальность данных**
   ```cpp
   // Плохо: обращение к памяти через строку
   for (size_t j = 0; j < cols; ++j) {
       for (size_t i = 0; i < rows; ++i) {
           matrix[i][j] = process(matrix[i][j]); // cache miss
       }
   }
   ```

2. **Ненужные копирования**
   ```cpp
   // Плохо: копирование больших объектов
   void process(std::vector<int> data) { // копия!
       // обработка
   }
   
   // Плохо: возврат по значению без move
   std::vector<int> create_vector() {
       std::vector<int> result;
       // заполнение
       return result; // может быть неэффективно в старых стандартах
   }
   ```

3. **Частые реаллокации**
   ```cpp
   // Плохо: вектор растёт экспоненциально
   std::vector<int> vec;
   for (int i = 0; i < 1000000; ++i) {
       vec.push_back(i); // множественные реаллокации
   }
   ```

4. **Избыточные вычисления**
   ```cpp
   // Плохо: повторное вычисление в цикле
   for (int i = 0; i < n; ++i) {
       for (int j = 0; j < expensive_function(); ++j) { // вызывается каждый раз
           // обработка
       }
   }
   ```

5. **Неэффективные условные переходы**
   ```cpp
   // Плохо: непредсказуемые ветвления
   for (int i = 0; i < n; ++i) {
       if (random_condition()) { // плохо для предсказателя веток
           process_a();
       } else {
           process_b();
       }
   }
   ```

## Современные инструменты профилирования (C++17)

### Встроенные средства C++17:

1. **std::chrono improvements**
   ```cpp
   // C++17: упрощённый синтаксис
   using namespace std::chrono_literals;
   auto start = std::chrono::steady_clock::now();
   std::this_thread::sleep_for(100ms);
   auto end = std::chrono::steady_clock::now();
   ```

2. **Структурированные привязки для результатов**
   ```cpp
   // C++17: удобное получение результатов
   auto [min_time, max_time, avg_time] = benchmark_function();
   ```

### Компилятор-специфичные инструменты:

1. **GCC/Clang встроенные функции**
   ```cpp
   // Получение количества тактов процессора
   uint64_t start_cycles = __builtin_readcyclecounter();
   // операция
   uint64_t end_cycles = __builtin_readcyclecounter();
   ```

2. **Атрибуты для оптимизации**
   ```cpp
   // C++17: подсказки компилятору
   [[likely]] if (common_condition) {
       // часто выполняемый код
   }
   [[unlikely]] else {
       // редко выполняемый код
   }
   ```

## Правила оптимизации

### Золотые правила:

1. **Правило 80/20 (Парето)**: 80% времени выполнения тратится на 20% кода
2. **Измеряйте, не предполагайте**: профилирование важнее интуиции
3. **Оптимизируйте узкие места**: сначала самые медленные части
4. **Читаемость vs производительность**: баланс между скоростью и пониманием кода
5. **Преждевременная оптимизация - корень всех зол**: сначала правильность, потом скорость

### Этапы оптимизации:

1. **Измерение**: определить текущую производительность
2. **Профилирование**: найти узкие места
3. **Анализ**: понять причины медленной работы
4. **Оптимизация**: применить конкретные техники
5. **Проверка**: убедиться в корректности и улучшении
6. **Повторение**: итеративный процесс улучшения

